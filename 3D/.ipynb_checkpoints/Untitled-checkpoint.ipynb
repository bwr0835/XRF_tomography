{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import xraylib as xlib\n",
    "import xraylib_np as xlib_np\n",
    "import torch as tc\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "matplotlib.rcParams['pdf.fonttype'] = 'truetype'\n",
    "fontProperties = {'family': 'serif', 'serif': ['Helvetica'], 'weight': 'normal', 'size': 12}\n",
    "plt.rc('font', **fontProperties)\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.ticker as mtick\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "from Atomic_number import AN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fl_K = np.array([xlib.KA1_LINE, xlib.KA2_LINE, xlib.KA3_LINE, xlib.KB1_LINE, xlib.KB2_LINE,\n",
    "#                  xlib.KB3_LINE, xlib.KB4_LINE, xlib.KB5_LINE])\n",
    "\n",
    "# fl_L = np.array([xlib.LA1_LINE, xlib.LA2_LINE, xlib.LB1_LINE, xlib.LB2_LINE, xlib.LB3_LINE,\n",
    "#                  xlib.LB4_LINE, xlib.LB5_LINE, xlib.LB6_LINE, xlib.LB7_LINE, xlib.LB9_LINE,\n",
    "#                  xlib.LB10_LINE, xlib.LB15_LINE, xlib.LB17_LINE])\n",
    "\n",
    "# fl_M = np.array([xlib.MA1_LINE, xlib.MA2_LINE, xlib.MB_LINE])\n",
    "\n",
    "# fl_line_groups = np.array([\"K\", \"L\", \"M\"])\n",
    "# group_lines = True\n",
    "# dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = {\"K\": np.array([xlib.KA1_LINE, xlib.KA2_LINE, xlib.KA3_LINE, xlib.KB1_LINE, xlib.KB2_LINE,\n",
    "                 xlib.KB3_LINE, xlib.KB4_LINE, xlib.KB5_LINE]),\n",
    "      \"L\": np.array([xlib.LA1_LINE, xlib.LA2_LINE, xlib.LB1_LINE, xlib.LB2_LINE, xlib.LB3_LINE,\n",
    "                 xlib.LB4_LINE, xlib.LB5_LINE, xlib.LB6_LINE, xlib.LB7_LINE, xlib.LB9_LINE,\n",
    "                 xlib.LB10_LINE, xlib.LB15_LINE, xlib.LB17_LINE]),              \n",
    "      \"M\": np.array([xlib.MA1_LINE, xlib.MA2_LINE, xlib.MB_LINE])               \n",
    "     }\n",
    "\n",
    "fl_line_groups = np.array([\"K\", \"L\", \"M\"])\n",
    "group_lines = True\n",
    "dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeFLlinesDictionary(this_aN_dic, probe_energy,\n",
    "                          sample_size_n, sample_size_cm,\n",
    "                          fl_line_groups = np.array([\"K\", \"L\", \"M\"]), fl_K = fl[\"K\"], fl_L = fl[\"L\"], fl_M = fl[\"M\"],\n",
    "                          group_lines = True):\n",
    "\n",
    "\n",
    "    element_ls = np.array(list(this_aN_dic.keys()))\n",
    "    aN_ls = np.array(list(this_aN_dic.values()))\n",
    "\n",
    "    n_line_group = len(fl_line_groups)\n",
    "    FL_all_elements_dic = {\"element_Line\": [], \"fl_energy\": np.array([]), \"detected_fl_unit_concentration\": np.array([])}\n",
    "    voxel_size = sample_size_cm/sample_size_n   \n",
    "\n",
    "    fl_cs_K = xlib_np.CS_FluorLine_Kissel_Cascade(aN_ls, fl_K, probe_energy)\n",
    "    fl_cs_L = xlib_np.CS_FluorLine_Kissel_Cascade(aN_ls, fl_L, probe_energy)\n",
    "    fl_cs_M = xlib_np.CS_FluorLine_Kissel_Cascade(aN_ls, fl_M, probe_energy)\n",
    "\n",
    "    # Remove the extra dimension with only 1 element\n",
    "    fl_cs_K = np.reshape(fl_cs_K, (fl_cs_K.shape[:-1]))\n",
    "    fl_cs_L = np.reshape(fl_cs_L, (fl_cs_L.shape[:-1]))\n",
    "    fl_cs_M = np.reshape(fl_cs_M, (fl_cs_M.shape[:-1]))\n",
    "\n",
    "    fl_energy_K = xlib_np.LineEnergy(aN_ls, fl_K)\n",
    "    fl_energy_L = xlib_np.LineEnergy(aN_ls, fl_L)\n",
    "    fl_energy_M = xlib_np.LineEnergy(aN_ls, fl_M)\n",
    "\n",
    "    FL_all_elements_dic = {\"(element_name, Line)\": [], \"fl_energy\": np.array([]), \"detected_fl_unit_concentration\": np.array([]),\n",
    "                           \"n_line_group_each_element\": np.array([]), \"n_lines\": None}\n",
    "    if group_lines == True:\n",
    "        fl_energy_group = np.zeros((len(element_ls),n_line_group))\n",
    "        fl_cs_group = np.zeros((len(element_ls),n_line_group))\n",
    "        \n",
    "        for i, element_name in enumerate(element_ls): \n",
    "\n",
    "            if np.sum(fl_cs_K[i] != 0):\n",
    "                fl_energy_group[i,0] = np.average(fl_energy_K[i], weights=fl_cs_K[i]) \n",
    "                fl_cs_group[i,0] = np.sum(fl_cs_K[i])\n",
    "            else:\n",
    "                fl_energy_group[i,0] = 0\n",
    "                fl_cs_group[i,0] = 0\n",
    "\n",
    "            if np.sum(fl_cs_L[i] != 0):\n",
    "                fl_energy_group[i,1] = np.average(fl_energy_L[i], weights=fl_cs_L[i]) \n",
    "                fl_cs_group[i,1] = np.sum(fl_cs_L[i])\n",
    "            else:\n",
    "                fl_energy_group[i,1] = 0\n",
    "                fl_cs_group[i,1] = 0\n",
    "\n",
    "            if np.sum(fl_cs_M[i] != 0):\n",
    "                fl_energy_group[i,2] = np.average(fl_energy_M[i], weights=fl_cs_M[i]) \n",
    "                fl_cs_group[i,2] = np.sum(fl_cs_M[i])\n",
    "            else:\n",
    "                fl_energy_group[i,2] = 0\n",
    "                fl_cs_group[i,2] = 0\n",
    "\n",
    "            element_Line = fl_line_groups[fl_energy_group[i]!= 0]\n",
    "            element_Line = [[element_name, element_Line[j]] for j in range(len(element_Line))]\n",
    "            for k in range(len(element_Line)):\n",
    "                FL_all_elements_dic[\"(element_name, Line)\"].append(element_Line[k])     \n",
    "\n",
    "            Line_energy = fl_energy_group[i][fl_energy_group[i]!=0]\n",
    "            FL_all_elements_dic[\"fl_energy\"] = np.append(FL_all_elements_dic[\"fl_energy\"], Line_energy)\n",
    "            fl_unit_con = fl_cs_group[i][fl_energy_group[i]!=0] * voxel_size\n",
    "            FL_all_elements_dic[\"detected_fl_unit_concentration\"] = np.append(FL_all_elements_dic[\"detected_fl_unit_concentration\"], fl_unit_con)\n",
    "            FL_all_elements_dic[\"n_line_group_each_element\"] = np.append(FL_all_elements_dic[\"n_line_group_each_element\"], len(fl_unit_con))\n",
    "            \n",
    "        FL_all_elements_dic[\"(element_name, Line)\"] = np.array(FL_all_elements_dic[\"(element_name, Line)\"])\n",
    "    \n",
    "    FL_all_elements_dic[\"n_lines\"] = len(FL_all_elements_dic[\"(element_name, Line)\"])\n",
    "    return FL_all_elements_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_aN_dic = {\"Ca\": 20, \"Sc\": 21}\n",
    "probe_energy = np.array([20.0])\n",
    "sample_size_n = tc.tensor(64).to(dev)\n",
    "sample_size_cm = tc.tensor(0.01).to(dev)\n",
    "\n",
    "FL_dic = MakeFLlinesDictionary(this_aN_dic, probe_energy,\n",
    "                          sample_size_n.cpu().numpy(), sample_size_cm.cpu().numpy(),\n",
    "                          fl_line_groups = np.array([\"K\", \"L\", \"M\"]), fl_K = fl[\"K\"], fl_L = fl[\"L\"], fl_M = fl[\"M\"],\n",
    "                          group_lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(element_name, Line)': array([['Ca', 'K'],\n",
       "        ['Ca', 'L'],\n",
       "        ['Sc', 'K'],\n",
       "        ['Sc', 'L']], dtype='<U2'),\n",
       " 'fl_energy': array([3.72645881, 0.33630868, 4.13132326, 0.39404391]),\n",
       " 'detected_fl_unit_concentration': array([3.00225145e-04, 2.54702887e-07, 3.77164143e-04, 1.71068821e-06]),\n",
       " 'n_line_group_each_element': array([2., 2.]),\n",
       " 'n_lines': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FL_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_aN_dic = {\"Al\": 13, \"Si\": 14, \"Cr\": 24, \"Cu\": 29}\n",
    "element_lines_roi = np.array([['Al', 'K'], ['Si', 'K'], ['Cr', 'K'], ['Cu', 'K']])\n",
    "n_line_group_each_element = np.array([1, 1, 1, 1])\n",
    "channel_names = 'exchange/elements' \n",
    "probe_energy = np.array([10.0])\n",
    "sample_size_n = 176\n",
    "sample_size_cm = 0.007\n",
    "data_path = './data/Xtal1_align1'\n",
    "f_XRF_data = 'xtal1_xrf-roi-plus'\n",
    "channel_names = 'exchange/elements' \n",
    "fl_K = fl[\"K\"]\n",
    "fl_L = fl[\"L\"]\n",
    "fl_M = fl[\"M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lines_roi_idx_from_dataset(data_path, f_XRF_data, element_lines_roi):\n",
    "    XRF_data = h5py.File(os.path.join(data_path, f_XRF_data), 'r')\n",
    "    channel_names = XRF_data['exchange/elements'][...]\n",
    "    channel_names = np.array([str(channel_name, 'utf-8') for channel_name in channel_names])\n",
    "\n",
    "    element_lines_roi_idx = np.zeros(len(element_lines_roi)).astype(np.int)\n",
    "    for i, element_line_roi in enumerate(element_lines_roi):\n",
    "        if element_line_roi[1] == \"K\":\n",
    "            channel_name_roi = element_line_roi[0]\n",
    "        else:\n",
    "            channel_name_roi = element_line_roi[0] + \"_\" + element_line_roi[1]\n",
    "        element_line_idx = np.argwhere(channel_names == channel_name_roi)\n",
    "        element_lines_roi_idx[i] = element_line_idx\n",
    "\n",
    "    return element_lines_roi_idx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  9 14]\n"
     ]
    }
   ],
   "source": [
    "element_lines_roi_idx = find_lines_roi_idx_from_dataset(data_path, f_XRF_data, element_lines_roi)\n",
    "print(element_lines_roi_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeFLlinesDictionary_manual(element_lines_roi,                           \n",
    "                                 n_line_group_each_element, probe_energy, \n",
    "                                 sample_size_n, sample_size_cm,\n",
    "                                 fl_line_groups = np.array([\"K\", \"L\", \"M\"]), fl_K = fl[\"K\"], fl_L = fl[\"L\"], fl_M = fl[\"M\"]):\n",
    "\n",
    "\n",
    "    FL_all_elements_dic = {\"(element_name, Line)\": [], \"fl_energy\": np.array([]), \"detected_fl_unit_concentration\": np.array([]),\n",
    "                           \"n_line_group_each_element\": np.array([]), \"n_lines\": None}\n",
    "\n",
    "    FL_all_elements_dic[\"(element_name, Line)\"] = element_lines_roi\n",
    "    FL_all_elements_dic[\"n_line_group_each_element\"] = n_line_group_each_element\n",
    "    FL_all_elements_dic[\"n_lines\"] = len(element_lines_roi)\n",
    "\n",
    "    voxel_size = sample_size_cm/sample_size_n   \n",
    "\n",
    "    for i, element_line_roi in enumerate(element_lines_roi):\n",
    "        fl_energy = xlib_np.LineEnergy(np.array([AN[element_line_roi[0]]]), fl[element_line_roi[1]]).flatten()\n",
    "        fl_cs = xlib_np.CS_FluorLine_Kissel_Cascade(np.array([AN[element_line_roi[0]]]), fl[element_line_roi[1]], probe_energy).flatten()\n",
    "\n",
    "        if np.sum(fl_cs) != 0:\n",
    "            fl_energy_group = np.average(fl_energy, weights=fl_cs) \n",
    "            fl_cs_group = np.sum(fl_cs)\n",
    "        else:\n",
    "            fl_energy_group = 0.\n",
    "            fl_cs_group = 0.\n",
    "\n",
    "        FL_all_elements_dic[\"fl_energy\"] = np.append(FL_all_elements_dic[\"fl_energy\"], fl_energy_group)\n",
    "        fl_unit_con = fl_cs_group * voxel_size\n",
    "        FL_all_elements_dic[\"detected_fl_unit_concentration\"] = np.append(FL_all_elements_dic[\"detected_fl_unit_concentration\"], fl_unit_con)   \n",
    "        \n",
    "    return FL_all_elements_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FL_dic = MakeFLlinesDictionary_manual(element_lines_roi,                           \n",
    "                                 n_line_group_each_element, probe_energy, \n",
    "                                 sample_size_n, sample_size_cm,\n",
    "                                 fl_line_groups = np.array([\"K\", \"L\", \"M\"]), fl_K = fl[\"K\"], fl_L = fl[\"L\"], fl_M = fl[\"M\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(element_name, Line)': array([['Al', 'K'],\n",
       "        ['Si', 'K'],\n",
       "        ['Cr', 'K'],\n",
       "        ['Cu', 'K']], dtype='<U2'),\n",
       " 'fl_energy': array([1.48761179, 1.74239384, 5.47259681, 8.14307424]),\n",
       " 'detected_fl_unit_concentration': array([3.66356306e-05, 6.14905727e-05, 1.40586946e-03, 3.37915546e-03]),\n",
       " 'n_line_group_each_element': array([1, 1, 1, 1]),\n",
       " 'n_lines': 4}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FL_dic\n",
    "# print(len(FL_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.,  5., 13., 31., 14., 14., 11.,  1.,  1.,  1.]),\n",
       " array([ 0. ,  1.3,  2.6,  3.9,  5.2,  6.5,  7.8,  9.1, 10.4, 11.7, 13. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = default_rng()\n",
    "a_noise = a.poisson(5 * np.ones(100))\n",
    "plt.hist(a_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 28835840)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('data/P_array/sample_64_64_64/detSpacing_0.4_dpts_5/backup4/Intersecting_Length_64_64_64.h5', 'r')\n",
    "f_data_array = np.array(f['P_array'][...])\n",
    "print(f_data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_data_array = np.load('data/P_array/sample_64_64_64/detSpacing_0.4_dpts_5/backup/Intersecting_Length_64_64_64.npy')\n",
    "# print(f_data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(arr, theta, dev):\n",
    "    \"\"\"\n",
    "    This function rotates the grid concentration with dimension: (n_element, sample_height_n, sample_size_n, sample_size_n)\n",
    "    The rotational axis is along dim 1 of the grid\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : torch tensor\n",
    "        grid concentration\n",
    "        \n",
    "    theta : float\n",
    "        rotation angle in radians (clockwise)\n",
    "    \n",
    "    dev : string\n",
    "        specify \"cpu\" or the cuda diveice (ex: cuda:0)\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q : torch tensor\n",
    "        the rotated grid concentration\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    m0 = tc.tensor([tc.cos(theta), -tc.sin(theta), 0.0], device=dev)\n",
    "    m1 = tc.tensor([tc.sin(theta), tc.cos(theta), 0.0], device=dev)\n",
    "    m = tc.stack([m0, m1]).view(1, 2, 3)\n",
    "    m = m.repeat([arr.shape[0], 1, 1])\n",
    "    \n",
    "    g = F.affine_grid(m, arr.shape)\n",
    "    q = F.grid_sample(arr, g, padding_mode='border')\n",
    "    \n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attenuation_3d(src_path, theta_st, theta_end, n_theta, sample_height_n, sample_size_n,\n",
    "                sample_size_cm, this_aN_dic, probe_energy, dev):\n",
    " \n",
    "    \n",
    "    n_element = len(this_aN_dic)\n",
    "    theta_ls = - tc.linspace(theta_st, theta_end, n_theta + 1)[:-1]\n",
    "    grid_concentration = tc.tensor(np.load(src_path)).float().to(dev)\n",
    "    aN_ls = np.array(list(this_aN_dic.values()))\n",
    "    probe_attCS_ls = tc.tensor(xlib_np.CS_Total(aN_ls, probe_energy).flatten()).float().to(dev)\n",
    "    \n",
    "    att_exponent_acc_map = tc.zeros((len(theta_ls), sample_height_n, sample_size_n, sample_size_n+1), device=dev)\n",
    "    for i , theta in enumerate(theta_ls):\n",
    "        theta = tc.tensor(theta,  device=dev)\n",
    "        concentration_map_rot = rotate(grid_concentration, theta, dev)\n",
    "        for j in range(n_element):\n",
    "            lac_single = concentration_map_rot[j] * probe_attCS_ls[j]\n",
    "            lac_acc = tc.cumsum(lac_single, axis=2)\n",
    "            lac_acc = tc.cat((tc.zeros((sample_height_n, sample_size_n, 1), device=dev), lac_acc), dim = 2)\n",
    "            att_exponent_acc = lac_acc * (sample_size_cm / sample_size_n) \n",
    "            att_exponent_acc_map[i,:,:,:] += att_exponent_acc\n",
    "\n",
    "    attenuation_map_flat = tc.exp(-(att_exponent_acc_map[:,:,:,:-1])).view(n_theta, sample_height_n * sample_size_n * sample_size_n).float().to(dev)\n",
    "    transmission = tc.exp(-att_exponent_acc_map[:,:,:,-1]).view(n_theta, sample_height_n * sample_size_n).float().to(dev)\n",
    "    \n",
    "    return attenuation_map_flat, transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"./data/sample8_size_64_pad/nElements_2/grid_concentration.npy\"\n",
    "theta_st = 0.\n",
    "theta_end = 2 * np.pi\n",
    "n_theta = 200\n",
    "sample_height_n = 64\n",
    "sample_size_n = 64\n",
    "sample_size_cm = 0.01\n",
    "this_aN_dic = {\"Ca\": 20, \"Sc\": 21}\n",
    "probe_energy = np.array([20.0])\n",
    "dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpphappy/anaconda3/envs/joint_XRF_XRT/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/home/hpphappy/anaconda3/envs/joint_XRF_XRT/lib/python3.7/site-packages/torch/nn/functional.py:3447: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n",
      "/home/hpphappy/anaconda3/envs/joint_XRF_XRT/lib/python3.7/site-packages/torch/nn/functional.py:3384: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "attenuation_map_flat, transmission = attenuation_3d(src_path, theta_st, theta_end, n_theta, sample_height_n, sample_size_n,\n",
    "                                                    sample_size_cm, this_aN_dic, probe_energy, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_XRT_data_3d(src_path, theta_st, theta_end, n_theta, sample_height_n, sample_size_n,\n",
    "                         sample_size_cm, this_aN_dic, probe_energy, probe_cts, save_path, save_fname, theta_sep, Poisson_noise, dev):\n",
    " \n",
    "    XRT_data = probe_cts * attenuation_3d(src_path, theta_st, theta_end, n_theta, sample_height_n, sample_size_n,\n",
    "                sample_size_cm, this_aN_dic, probe_energy, dev)[1]\n",
    "    \n",
    "    if Poisson_noise == True:\n",
    "        random_noise_generator = default_rng()\n",
    "        XRT_data = random_noise_generator.poisson(XRT_data)\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    else:\n",
    "        pass    \n",
    "    \n",
    "    if theta_sep == True:       \n",
    "        for this_theta_idx in tqdm(range(n_theta)):\n",
    "            np.save(os.path.join(save_path, save_fname +'_{}'.format(this_theta_idx)), XRT_data[this_theta_idx])\n",
    "    \n",
    "    else:\n",
    "        np.save(os.path.join(save_path, save_fname), XRT_data.cpu())\n",
    "    \n",
    "    return XRT_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 262144])\n",
      "torch.Size([200, 4096])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"./data/P_array/sample_44_44_20/detSize_2.4_detSpacing_1.2_dpts_5/backup/Intersecting_Length_44_44_20.h5\", \"r\") as f:\n",
    "    P_array_1 = f['P_array']\n",
    "\n",
    "with h5py.File(\"./data/P_array/sample_44_44_20/detSize_2.4_detSpacing_1.2_dpts_5/Intersecting_Length_44_44_20.h5\", \"r\") as f:\n",
    "    P_array_2 = f['P_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 2 2 2]\n",
      " [3 3 3 3 3 3 3 3 3 3]\n",
      " [4 4 4 4 4 4 4 4 4 4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[n]*10 for n in range(5)])\n",
    "print(a)\n",
    "b = np.array([0.,4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3., 3., 3.]])\n",
      "tensor([1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "a = tc.tensor([1,2,3]).view(3,1)\n",
    "b = a * tc.ones(3,6)\n",
    "c = (a * tc.ones(3,6)).view(18)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.arange(2,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_n = 8\n",
    "sample_height_n = 10\n",
    "\n",
    "z_s, x_s, y_s = np.indices((int(sample_height_n), int(sample_size_n), int(sample_size_n))) + 0.5\n",
    "voxel_pos_ls_flat = np.stack((z_s.flatten(), x_s.flatten(), y_s.flatten()), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 8, 8)\n",
      "(640, 3)\n"
     ]
    }
   ],
   "source": [
    "print(z_s.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "print(voxel_pos_ls_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
